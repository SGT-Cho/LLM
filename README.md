
# ğŸ§  Large Language Models (LLMs): Fundamentals and Enhancement Techniques

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

LLM ê¸°ìˆ ì˜ í•µì‹¬ ê°œë…ê³¼ í–¥ìƒ ê¸°ë²•ì„ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•œ ì €ì¥ì†Œì…ë‹ˆë‹¤. ì´ë¡  ì„¤ëª…ê³¼ ì‹¤ì œ êµ¬í˜„ ì˜ˆì‹œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

## ğŸ“š Table of Contents
1. [LLM Overview](#-llm-overview)
2. [Enhancement Techniques](#-enhancement-techniques)
3. [Contributing](#-contributing)
4. [References](#-references)

---

## ğŸ§© LLM Overview

### What are LLMs?
ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ í•™ìŠµëœ ë”¥ëŸ¬ë‹ ëª¨ë¸ë¡œ, ìì—°ì–´ ì´í•´ ë° ìƒì„± ì‘ì—…ì—ì„œ ì¸ê°„ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.

### Model Types
| ì¢…ë¥˜          | ì˜ˆì‹œ ëª¨ë¸       | íŠ¹ì§•                          |
|---------------|-----------------|------------------------------|
| Autoregressive| GPT-4, LLaMA    | ìˆœì°¨ì  í…ìŠ¤íŠ¸ ìƒì„±            |
| Autoencoder   | BERT, RoBERTa   | ì–‘ë°©í–¥ ë¬¸ë§¥ ì´í•´              |
| Multimodal    | CLIP, Flamingo  | í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ì²˜ë¦¬            |

---

## ğŸ›  Enhancement Techniques

### 1. RAG (Retrieval-Augmented Generation)
ğŸ” **ì§€ì‹ ì¦ê°• ìƒì„±**
- ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì™€ ê²°í•©í•˜ì—¬ ì •í™•ë„ í–¥ìƒ
- êµ¬í˜„ í”„ë ˆì„ì›Œí¬: LangChain, Haystack

1. **ì§€ì‹ ë² ì´ìŠ¤ ëª…ì‹œì  í‘œê¸°** 
2. **ì‹¤ì œ ë™ì‘ ë°©ì‹ ë°˜ì˜**
3. **í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ê°•ì¡°**

```mermaid
graph TD
    A[User Question] --> B(Query Embedding)
    B --> C{Vector DB Search}
    C --> D[Top-k Documents]
    D --> E[Context Filtering]
    E --> F[Re-ranking]
    F --> G{LLM Generator}
    G --> H[Final Answer]
    
    subgraph Knowledge Base
        C -->|ì—°ê²°| I[(Chunk Storage)]
        I --> J[Metadata]
        I --> K[Text Embeddings]
    end
    
    G -->|ìš”ì²­| M[External APIs]
    H -->|Feedback| A
```

### 2. Fine-tuning
ğŸ¯ **ë„ë©”ì¸ íŠ¹í™” í•™ìŠµ**
- ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ íŠ¹ì • ì‘ì—…ì— ë§ì¶° ì¡°ì •
- ì£¼ìš” ê¸°ë²•:
  - Full Fine-tuning
  - LoRA (Low-Rank Adaptation)
  - Prompt Tuning

### 3. Quantization
âš–ï¸ **ëª¨ë¸ ê²½ëŸ‰í™”**
- FP32 â†’ INT8 ë³€í™˜ìœ¼ë¡œ 4ë°° ê²½ëŸ‰í™”
- ì¶”ë¡  ì†ë„ 2-3ë°° í–¥ìƒ

### 4. Multimodal Integration
ğŸŒ **ë©€í‹°í‹° ëª¨ë‹¬ í†µí•©**
- í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€/ë¹„ë””ì˜¤/ì˜¤ë””ì˜¤ ì²˜ë¦¬
- ì£¼ìš” ì•„í‚¤í…ì²˜:
  - Cross-modal Attention
  - Fusion Networks

---
  


## ğŸ“š References
- [Attention Is All You Need (2017)](https://arxiv.org/abs/1706.03762)
- [LoRA: Low-Rank Adaptation of Large Language Models (2021)](https://arxiv.org/abs/2106.09685)
- [Hugging Face Transformers Library](https://huggingface.co/docs/transformers)
